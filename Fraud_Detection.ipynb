{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9ag2tU96+rZfu65OqhgVF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Setup & installs"],"metadata":{"id":"Wqp8yA0aHRy4"}},{"cell_type":"code","source":["# Colab: install extra libraries\n","!pip install -q imbalanced-learn xgboost fastapi uvicorn[standard] joblib\n"],"metadata":{"id":"W3C9x302HUlu","executionInfo":{"status":"ok","timestamp":1764914722841,"user_tz":-330,"elapsed":5412,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Core libraries\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.pipeline import Pipeline as ImbPipeline\n","\n","import joblib\n"],"metadata":{"id":"ZkpFw0shHZw-","executionInfo":{"status":"ok","timestamp":1764914738875,"user_tz":-330,"elapsed":51,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# 2. Load dataset"],"metadata":{"id":"XjnTiAocHdMQ"}},{"cell_type":"code","source":["from google.colab import files\n","import zipfile, os, glob\n","import pandas as pd\n","\n","# 1) Upload the zip file (choose dataset.zip)\n","uploaded = files.upload()\n","\n","# 2) Get the name of the uploaded zip\n","zip_name = next(iter(uploaded))   # e.g. \"dataset.zip\"\n","print(\"Uploaded file:\", zip_name)\n","\n","# 3) Extract it into /content/data\n","extract_dir = \"/content/data\"\n","os.makedirs(extract_dir, exist_ok=True)\n","\n","with zipfile.ZipFile(zip_name, \"r\") as z:\n","    z.extractall(extract_dir)\n","\n","print(\"\\nExtracted contents:\\n\")\n","for root, dirs, files_in_dir in os.walk(extract_dir):\n","    print(root)\n","    for f in files_in_dir:\n","        print(\"   \", f)\n","\n","# 4) Find all .pkl files (your fraud data is stored as daily .pkl files)\n","pkl_files = sorted(glob.glob(os.path.join(extract_dir, \"**\", \"*.pkl\"),\n","                             recursive=True))\n","\n","if len(pkl_files) == 0:\n","    raise ValueError(\"No .pkl files found inside the uploaded zip. Check the zip structure.\")\n","else:\n","    print(f\"\\nFound {len(pkl_files)} pickle files. Example:\")\n","    print(pkl_files[:5])\n","\n","# 5) Load and concatenate all .pkl files into a single DataFrame\n","dfs = [pd.read_pickle(p) for p in pkl_files]\n","df = pd.concat(dfs, ignore_index=True)\n","\n","print(\"\\nFinal DataFrame:\")\n","print(\"Shape:\", df.shape)\n","print(df.head())\n","print(df.columns)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"YXe5k89wLsi3","executionInfo":{"status":"ok","timestamp":1764916372020,"user_tz":-330,"elapsed":113021,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}},"outputId":"5c80001e-5b97-4a07-fec8-f73eeb78e5bb"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-9b9adfd2-80c9-441e-aa70-a905328d6f65\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-9b9adfd2-80c9-441e-aa70-a905328d6f65\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving dataset.zip to dataset (4).zip\n","Uploaded file: dataset (4).zip\n","\n","Extracted contents:\n","\n","/content/data\n","/content/data/data\n","    2018-09-28.pkl\n","    2018-05-24.pkl\n","    2018-07-16.pkl\n","    2018-06-16.pkl\n","    2018-07-11.pkl\n","    2018-09-30.pkl\n","    2018-08-01.pkl\n","    2018-06-20.pkl\n","    2018-05-21.pkl\n","    2018-04-15.pkl\n","    2018-08-22.pkl\n","    2018-07-24.pkl\n","    2018-06-30.pkl\n","    2018-08-16.pkl\n","    2018-07-15.pkl\n","    2018-08-29.pkl\n","    2018-07-21.pkl\n","    2018-04-18.pkl\n","    2018-07-01.pkl\n","    2018-08-02.pkl\n","    2018-05-04.pkl\n","    2018-09-23.pkl\n","    2018-09-10.pkl\n","    2018-04-17.pkl\n","    2018-05-07.pkl\n","    2018-07-13.pkl\n","    2018-08-20.pkl\n","    2018-08-28.pkl\n","    2018-07-26.pkl\n","    2018-05-01.pkl\n","    2018-04-05.pkl\n","    2018-04-29.pkl\n","    2018-08-12.pkl\n","    2018-06-19.pkl\n","    2018-08-06.pkl\n","    2018-05-15.pkl\n","    2018-05-20.pkl\n","    2018-06-14.pkl\n","    2018-04-08.pkl\n","    2018-09-05.pkl\n","    2018-08-25.pkl\n","    2018-05-05.pkl\n","    2018-06-08.pkl\n","    2018-04-13.pkl\n","    2018-04-21.pkl\n","    2018-04-01.pkl\n","    2018-06-12.pkl\n","    2018-08-14.pkl\n","    2018-07-17.pkl\n","    2018-05-11.pkl\n","    2018-05-12.pkl\n","    2018-07-29.pkl\n","    2018-06-04.pkl\n","    2018-04-02.pkl\n","    2018-08-21.pkl\n","    2018-05-06.pkl\n","    2018-07-02.pkl\n","    2018-09-12.pkl\n","    2018-05-26.pkl\n","    2018-05-08.pkl\n","    2018-07-28.pkl\n","    2018-05-10.pkl\n","    2018-04-27.pkl\n","    2018-07-20.pkl\n","    2018-04-25.pkl\n","    2018-06-13.pkl\n","    2018-07-08.pkl\n","    2018-04-11.pkl\n","    2018-09-25.pkl\n","    2018-07-30.pkl\n","    2018-05-03.pkl\n","    2018-04-30.pkl\n","    2018-09-11.pkl\n","    2018-08-31.pkl\n","    2018-05-18.pkl\n","    2018-09-09.pkl\n","    2018-06-02.pkl\n","    2018-07-31.pkl\n","    2018-06-21.pkl\n","    2018-06-23.pkl\n","    2018-05-30.pkl\n","    2018-08-23.pkl\n","    2018-05-19.pkl\n","    2018-09-17.pkl\n","    2018-05-31.pkl\n","    2018-08-13.pkl\n","    2018-04-24.pkl\n","    2018-06-07.pkl\n","    2018-09-06.pkl\n","    2018-08-04.pkl\n","    2018-04-23.pkl\n","    2018-08-15.pkl\n","    2018-08-17.pkl\n","    2018-09-07.pkl\n","    2018-06-03.pkl\n","    2018-05-22.pkl\n","    2018-07-12.pkl\n","    2018-06-27.pkl\n","    2018-08-30.pkl\n","    2018-07-06.pkl\n","    2018-08-08.pkl\n","    2018-07-07.pkl\n","    2018-06-06.pkl\n","    2018-05-14.pkl\n","    2018-07-05.pkl\n","    2018-06-22.pkl\n","    2018-07-27.pkl\n","    2018-04-19.pkl\n","    2018-06-10.pkl\n","    2018-06-15.pkl\n","    2018-06-09.pkl\n","    2018-04-10.pkl\n","    2018-06-24.pkl\n","    2018-09-14.pkl\n","    2018-09-13.pkl\n","    2018-05-25.pkl\n","    2018-06-26.pkl\n","    2018-04-14.pkl\n","    2018-08-05.pkl\n","    2018-09-20.pkl\n","    2018-04-06.pkl\n","    2018-09-18.pkl\n","    2018-04-20.pkl\n","    2018-09-27.pkl\n","    2018-07-22.pkl\n","    2018-04-04.pkl\n","    2018-08-19.pkl\n","    2018-05-28.pkl\n","    2018-05-16.pkl\n","    2018-04-26.pkl\n","    2018-09-15.pkl\n","    2018-07-25.pkl\n","    2018-09-22.pkl\n","    2018-09-08.pkl\n","    2018-07-04.pkl\n","    2018-06-18.pkl\n","    2018-04-16.pkl\n","    2018-05-29.pkl\n","    2018-05-13.pkl\n","    2018-08-27.pkl\n","    2018-08-11.pkl\n","    2018-08-07.pkl\n","    2018-07-14.pkl\n","    2018-04-07.pkl\n","    2018-07-23.pkl\n","    2018-04-09.pkl\n","    2018-09-26.pkl\n","    2018-09-21.pkl\n","    2018-08-18.pkl\n","    2018-06-17.pkl\n","    2018-07-09.pkl\n","    2018-09-24.pkl\n","    2018-09-29.pkl\n","    2018-07-10.pkl\n","    2018-04-28.pkl\n","    2018-05-02.pkl\n","    2018-08-24.pkl\n","    2018-09-19.pkl\n","    2018-07-18.pkl\n","    2018-09-03.pkl\n","    2018-06-05.pkl\n","    2018-04-12.pkl\n","    2018-06-25.pkl\n","    2018-05-09.pkl\n","    2018-06-29.pkl\n","    2018-06-28.pkl\n","    2018-08-03.pkl\n","    2018-07-03.pkl\n","    2018-06-01.pkl\n","    2018-05-23.pkl\n","    2018-09-01.pkl\n","    2018-04-03.pkl\n","    2018-08-10.pkl\n","    2018-09-02.pkl\n","    2018-04-22.pkl\n","    2018-09-16.pkl\n","    2018-07-19.pkl\n","    2018-05-27.pkl\n","    2018-06-11.pkl\n","    2018-08-09.pkl\n","    2018-09-04.pkl\n","    2018-08-26.pkl\n","    2018-05-17.pkl\n","\n","Found 183 pickle files. Example:\n","['/content/data/data/2018-04-01.pkl', '/content/data/data/2018-04-02.pkl', '/content/data/data/2018-04-03.pkl', '/content/data/data/2018-04-04.pkl', '/content/data/data/2018-04-05.pkl']\n","\n","Final DataFrame:\n","Shape: (1754155, 9)\n","   TRANSACTION_ID         TX_DATETIME CUSTOMER_ID TERMINAL_ID  TX_AMOUNT  \\\n","0               0 2018-04-01 00:00:31         596        3156      57.16   \n","1               1 2018-04-01 00:02:10        4961        3412      81.51   \n","2               2 2018-04-01 00:07:56           2        1365     146.00   \n","3               3 2018-04-01 00:09:29        4128        8737      64.49   \n","4               4 2018-04-01 00:10:34         927        9906      50.99   \n","\n","  TX_TIME_SECONDS TX_TIME_DAYS  TX_FRAUD  TX_FRAUD_SCENARIO  \n","0              31            0         0                  0  \n","1             130            0         0                  0  \n","2             476            0         0                  0  \n","3             569            0         0                  0  \n","4             634            0         0                  0  \n","Index(['TRANSACTION_ID', 'TX_DATETIME', 'CUSTOMER_ID', 'TERMINAL_ID',\n","       'TX_AMOUNT', 'TX_TIME_SECONDS', 'TX_TIME_DAYS', 'TX_FRAUD',\n","       'TX_FRAUD_SCENARIO'],\n","      dtype='object')\n"]}]},{"cell_type":"markdown","source":["# 3. Parse datetime & sort + basic time-based split"],"metadata":{"id":"NLRFsoPzbIS6"}},{"cell_type":"code","source":["# Parse TX_DATETIME and sort\n","df['TX_DATETIME'] = pd.to_datetime(df['TX_DATETIME'])\n","df = df.sort_values('TX_DATETIME').reset_index(drop=True)\n","\n","# Choose last 30 days as test set (you can tweak this)\n","max_date = df['TX_DATETIME'].max().normalize()\n","split_date = max_date - pd.Timedelta(days=30)\n","\n","train_df = df[df['TX_DATETIME'] < split_date].copy()\n","test_df  = df[df['TX_DATETIME'] >= split_date].copy()\n","\n","print(\"Train shape:\", train_df.shape, \"Test shape:\", test_df.shape)\n","print(\"Train fraud rate:\", train_df['TX_FRAUD'].mean())\n","print(\"Test fraud rate:\", test_df['TX_FRAUD'].mean())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQ3w7IEybLME","executionInfo":{"status":"ok","timestamp":1764919925621,"user_tz":-330,"elapsed":3080,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}},"outputId":"e3cea462-eb28-473d-ba3a-9ff0ad49b103"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (1456778, 9) Test shape: (297377, 9)\n","Train fraud rate: 0.008268246774731633\n","Test fraud rate: 0.008864169051406127\n"]}]},{"cell_type":"markdown","source":["# 4. Feature engineering – velocity & aggregates\n","We’ll add:\n","\n","\n","\n","1.   Transaction hour, day of week\n","\n","2.  Customer 1-day transaction count & total amount\n","\n","3.   Terminal 1-day transaction count\n","\n"],"metadata":{"id":"BQOD5YLUbPBP"}},{"cell_type":"code","source":["\n","# 1) Add simple time features (hour, day-of-week, day)\n","def add_time_features(data):\n","    data['TX_HOUR'] = data['TX_DATETIME'].dt.hour\n","    data['TX_DOW']  = data['TX_DATETIME'].dt.dayofweek\n","    data['TX_DAY']  = data['TX_DATETIME'].dt.date\n","    return data\n","\n","train_df = add_time_features(train_df)\n","test_df  = add_time_features(test_df)\n","\n","\n","# 2) Add velocity features for a given key (CUSTOMER_ID / TERMINAL_ID)\n","def add_velocity_features(data, key_col, amount_col='TX_AMOUNT', window='1D'):\n","    \"\"\"\n","    For each key (customer/terminal), compute in the past 'window':\n","    - number of past transactions (key_col_1d_txn_count)\n","    - total past transaction amount (key_col_1d_txn_amount)\n","\n","    Uses groupby().rolling(on='TX_DATETIME') to avoid index issues.\n","    \"\"\"\n","    # Sort by time so rolling is chronological\n","    data = data.sort_values(['TX_DATETIME'])\n","\n","    # Rolling window per key, using TX_DATETIME as time column\n","    grouped = (\n","        data\n","        .groupby(key_col)\n","        .rolling(window=window, on='TX_DATETIME')[amount_col]\n","    )\n","\n","    # grouped.count() / grouped.sum() include current txn -> subtract current\n","    count_roll = grouped.count().values - 1\n","    sum_roll   = grouped.sum().values - data[amount_col].values\n","\n","    # Assign back (order matches data after .values)\n","    data[f'{key_col}_1d_txn_count']  = count_roll\n","    data[f'{key_col}_1d_txn_amount'] = sum_roll\n","\n","    # No past history -> 0\n","    data[f'{key_col}_1d_txn_count']  = data[f'{key_col}_1d_txn_count'].fillna(0)\n","    data[f'{key_col}_1d_txn_amount'] = data[f'{key_col}_1d_txn_amount'].fillna(0)\n","\n","    return data\n","\n","\n","# 3) Apply velocity features first on train_df (for inspection, if needed)\n","train_df = add_velocity_features(train_df, 'CUSTOMER_ID')\n","train_df = add_velocity_features(train_df, 'TERMINAL_ID')\n","\n","# 4) Recompute velocity features on full data to avoid leakage,\n","#    then re-split into train and test\n","full_df = pd.concat([train_df, test_df], axis=0)\n","full_df = full_df.sort_values('TX_DATETIME').reset_index(drop=True)\n","\n","full_df = add_velocity_features(full_df, 'CUSTOMER_ID')\n","full_df = add_velocity_features(full_df, 'TERMINAL_ID')\n","\n","# 5) Final train/test after adding all features\n","train_df = full_df[full_df['TX_DATETIME'] < split_date].copy()\n","test_df  = full_df[full_df['TX_DATETIME'] >= split_date].copy()\n","\n","print(\"Train shape with features:\", train_df.shape)\n","print(\"Test shape with features:\", test_df.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWxXO9tibgrn","executionInfo":{"status":"ok","timestamp":1764923131755,"user_tz":-330,"elapsed":19008,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}},"outputId":"49b37237-415f-41cb-a154-10f9432e602e"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape with features: (1456778, 16)\n","Test shape with features: (297377, 16)\n"]}]},{"cell_type":"markdown","source":["# 5. Prepare features & encoders\n","We’ll label-encode IDs and scale numeric features."],"metadata":{"id":"fbVlzFh1nmWM"}},{"cell_type":"code","source":["# Target\n","y_train = train_df['TX_FRAUD']\n","y_test  = test_df['TX_FRAUD']\n","\n","# Feature columns\n","numeric_cols = [\n","    'TX_AMOUNT', 'TX_HOUR', 'TX_DOW',\n","    'CUSTOMER_ID_1d_txn_count', 'CUSTOMER_ID_1d_txn_amount',\n","    'TERMINAL_ID_1d_txn_count', 'TERMINAL_ID_1d_txn_amount'\n","]\n","\n","# Fit LabelEncoders on ALL IDs (train + test) to avoid \"unseen label\" errors\n","cust_le = LabelEncoder()\n","term_le = LabelEncoder()\n","\n","all_cust_ids = pd.concat([train_df['CUSTOMER_ID'], test_df['CUSTOMER_ID']])\n","all_term_ids = pd.concat([train_df['TERMINAL_ID'], test_df['TERMINAL_ID']])\n","\n","cust_le.fit(all_cust_ids)\n","term_le.fit(all_term_ids)\n","\n","train_df['CUSTOMER_ID_ENC'] = cust_le.transform(train_df['CUSTOMER_ID'])\n","test_df['CUSTOMER_ID_ENC']  = cust_le.transform(test_df['CUSTOMER_ID'])\n","\n","train_df['TERMINAL_ID_ENC'] = term_le.transform(train_df['TERMINAL_ID'])\n","test_df['TERMINAL_ID_ENC']  = term_le.transform(test_df['TERMINAL_ID'])\n","\n","categorical_enc_cols = ['CUSTOMER_ID_ENC', 'TERMINAL_ID_ENC']\n","\n","X_train = train_df[categorical_enc_cols + numeric_cols]\n","X_test  = test_df[categorical_enc_cols + numeric_cols]\n","\n","# Preprocessor: scale numeric, pass IDs through\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numeric_cols),\n","        ('cat', 'passthrough', categorical_enc_cols),\n","    ]\n",")"],"metadata":{"id":"eW9Kw90inr9A","executionInfo":{"status":"ok","timestamp":1764923452264,"user_tz":-330,"elapsed":2123,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# 6. Helper to train & evaluate models"],"metadata":{"id":"dLouKLVOotV2"}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve\n","def evaluate_model(name, model, X_train, y_train, X_test, y_test):\n","    model.fit(X_train, y_train)\n","    y_proba = model.predict_proba(X_test)[:, 1]\n","    y_pred  = (y_proba >= 0.5).astype(int)\n","\n","    auc = roc_auc_score(y_test, y_proba)\n","    print(f\"\\n==== {name} ====\")\n","    print(\"ROC-AUC:\", round(auc, 4))\n","    print(\"\\nClassification report:\")\n","    print(classification_report(y_test, y_pred, digits=4))\n","    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n","\n","    return auc"],"metadata":{"id":"40PUKNUzpA7_","executionInfo":{"status":"ok","timestamp":1764923568421,"user_tz":-330,"elapsed":442,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# 7. Models with imbalance handling\n","7.1 Logistic Regression with class weights"],"metadata":{"id":"5qjAEIi7pH0z"}},{"cell_type":"code","source":["log_reg_clf = Pipeline(steps=[\n","    ('preprocess', preprocessor),\n","    ('clf', LogisticRegression(\n","        max_iter=2000,\n","        class_weight='balanced',  # handles imbalance\n","        n_jobs=-1\n","    ))\n","])\n","\n","auc_logreg = evaluate_model(\"Logistic Regression (class_weight=balanced)\",\n","                            log_reg_clf, X_train, y_train, X_test, y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZQ_A2IXpSWX","executionInfo":{"status":"ok","timestamp":1764923717613,"user_tz":-330,"elapsed":91228,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}},"outputId":"7d267b2a-ff86-4686-89ac-09640b7f92e8"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==== Logistic Regression (class_weight=balanced) ====\n","ROC-AUC: 0.6491\n","\n","Classification report:\n","              precision    recall  f1-score   support\n","\n","           0     0.9937    0.7560    0.8587    294741\n","           1     0.0168    0.4651    0.0324      2636\n","\n","    accuracy                         0.7535    297377\n","   macro avg     0.5052    0.6106    0.4455    297377\n","weighted avg     0.9851    0.7535    0.8514    297377\n","\n","Confusion matrix:\n"," [[222835  71906]\n"," [  1410   1226]]\n"]}]},{"cell_type":"markdown","source":["\n","7.2 RandomForest + undersampling"],"metadata":{"id":"kH1Gh1bFplui"}},{"cell_type":"code","source":["rf = RandomForestClassifier(\n","    n_estimators=200,\n","    max_depth=None,\n","    n_jobs=-1,\n","    class_weight=None,  # we'll use undersampling instead\n","    random_state=42\n",")\n","\n","rf_pipeline = ImbPipeline(steps=[\n","    ('preprocess', preprocessor),\n","    ('undersample', RandomUnderSampler(random_state=42)),\n","    ('clf', rf)\n","])\n","\n","auc_rf = evaluate_model(\"RandomForest + RandomUnderSampler\",\n","                        rf_pipeline, X_train, y_train, X_test, y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXb-kDhMpp9L","executionInfo":{"status":"ok","timestamp":1764923768358,"user_tz":-330,"elapsed":35536,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}},"outputId":"d7cd431d-a6a5-43e6-b338-f7d3816e77d9"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==== RandomForest + RandomUnderSampler ====\n","ROC-AUC: 0.6327\n","\n","Classification report:\n","              precision    recall  f1-score   support\n","\n","           0     0.9934    0.7891    0.8795    294741\n","           1     0.0173    0.4162    0.0333      2636\n","\n","    accuracy                         0.7858    297377\n","   macro avg     0.5054    0.6026    0.4564    297377\n","weighted avg     0.9848    0.7858    0.8720    297377\n","\n","Confusion matrix:\n"," [[232572  62169]\n"," [  1539   1097]]\n"]}]},{"cell_type":"markdown","source":["\n","7.3 XGBoost + SMOTE"],"metadata":{"id":"HL_lWEKapyzy"}},{"cell_type":"code","source":["xgb = XGBClassifier(\n","    n_estimators=300,\n","    max_depth=5,\n","    learning_rate=0.05,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    objective='binary:logistic',\n","    eval_metric='auc',\n","    n_jobs=-1,\n","    random_state=42\n",")\n","\n","xgb_pipeline = ImbPipeline(steps=[\n","    ('preprocess', preprocessor),\n","    ('smote', SMOTE(random_state=42)),\n","    ('clf', xgb)\n","])\n","\n","auc_xgb = evaluate_model(\"XGBoost + SMOTE\",\n","                         xgb_pipeline, X_train, y_train, X_test, y_test)\n","\n","print(\"\\nAUC summary:\")\n","print(\"LogReg:\", auc_logreg)\n","print(\"RF    :\", auc_rf)\n","print(\"XGB   :\", auc_xgb)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"No24Qm1up2mv","executionInfo":{"status":"ok","timestamp":1764923849944,"user_tz":-330,"elapsed":67424,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}},"outputId":"2b170322-b1b2-4ed9-d7c8-f0a1986fd7e1"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==== XGBoost + SMOTE ====\n","ROC-AUC: 0.6425\n","\n","Classification report:\n","              precision    recall  f1-score   support\n","\n","           0     0.9930    0.9999    0.9965    294741\n","           1     0.9686    0.2105    0.3459      2636\n","\n","    accuracy                         0.9929    297377\n","   macro avg     0.9808    0.6052    0.6712    297377\n","weighted avg     0.9928    0.9929    0.9907    297377\n","\n","Confusion matrix:\n"," [[294723     18]\n"," [  2081    555]]\n","\n","AUC summary:\n","LogReg: 0.6490699179685131\n","RF    : 0.6327111990698204\n","XGB   : 0.6425251393138203\n"]}]},{"cell_type":"markdown","source":["# 8. Save the best model & encoders"],"metadata":{"id":"tNm4R19uqQXt"}},{"cell_type":"code","source":["BEST_MODEL_PATH = \"/content/fraud_model_xgb.joblib\"\n","ID_ENCODERS_PATH = \"/content/id_encoders.joblib\"\n","\n","joblib.dump(xgb_pipeline, BEST_MODEL_PATH)\n","joblib.dump(\n","    {\n","        \"cust_le\": cust_le,\n","        \"term_le\": term_le\n","    },\n","    ID_ENCODERS_PATH\n",")\n","\n","print(\"Saved model to:\", BEST_MODEL_PATH)\n","print(\"Saved encoders to:\", ID_ENCODERS_PATH)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GbTM8qnmqZAK","executionInfo":{"status":"ok","timestamp":1764923912976,"user_tz":-330,"elapsed":444,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}},"outputId":"28101c6a-5199-4459-9f2b-a7e6c5e1df11"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved model to: /content/fraud_model_xgb.joblib\n","Saved encoders to: /content/id_encoders.joblib\n"]}]},{"cell_type":"markdown","source":["# Export model from Google Colab"],"metadata":{"id":"cLlArbNntQu3"}},{"cell_type":"code","source":["from google.colab import files\n","\n","files.download('/content/fraud_model_xgb.joblib')\n","files.download('/content/id_encoders.joblib')\n"],"metadata":{"id":"V7n98NWZtU5P","executionInfo":{"status":"ok","timestamp":1764924681393,"user_tz":-330,"elapsed":453,"user":{"displayName":"pardhiv aryan vempati","userId":"16561983999128131681"}},"outputId":"f004dcf7-9b11-4b43-e475-293328aac61e","colab":{"base_uri":"https://localhost:8080/","height":17}},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_8c4dffe0-c8df-45d4-89a9-66c69d8fcee8\", \"fraud_model_xgb.joblib\", 2698532)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_afd0eb53-9d20-467c-9274-699b37a1ea16\", \"id_encoders.joblib\", 45194)"]},"metadata":{}}]}]}